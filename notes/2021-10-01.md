---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Missing Data and inconsistent coding

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
import numpy as np #
na_toy_df = pd.DataFrame(data = [[1,3,4,5],[2 ,6, np.nan]])

# make plots look nicer and increase font size
sns.set_theme(font_scale=2)
arabica_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv'

coffee_df = pd.read_csv(arabica_data_url)


rhodyprog4ds_gh_events_url = 'https://api.github.com/orgs/rhodyprog4ds/events'
course_gh_df = pd.read_json(rhodyprog4ds_gh_events_url)
```

So far, we've dealt with structural issues in data. but there's a lot more to
cleaning.  

Today,  we'll deal with how to fix the values wihtin  the data.  To see the
types of things:

[Stanford Policy Lab Open Policing Project data readme](https://github.com/stanford-policylab/opp/blob/master/data_readme.md)
[Propublica Machine Bias](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) the "How we acquired data" section

+++

## Missing Values


Dealing with missing data is a whole research area. There isn't one solution.

[in 2020 there was a workshop on it](https://artemiss-workshop.github.io/)

There are also many classic approaches both when training and when [applying models](https://www.jmlr.org/papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf).

[example application in breast cancer detection](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.701.4234&rep=rep1&type=pdf)

Even in pandas, dealing with [missing values](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html) is under [experimentaion](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data-na)
 as to how to represent it symbolically

 Missing values even causes the [datatypes to change](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data-casting-rules-and-indexing)

 Pandas gives a few basic tools:

 - dropna
 - fillna

```{code-cell} ipython3
coffee_df.head()
```

```{code-cell} ipython3
coffee_df['Lot.Number'].dtype
```

```{code-cell} ipython3
coffee_df['Lot.Number'].value_counts()
```

```{code-cell} ipython3
coffee_df['Lot.Number'].fillna(1)
```

```{code-cell} ipython3
coffee_df['Lot.Number']
```

Filling can be good if you know how to fill reasonably, but don't have data to
spare by dropping.  For example
- you can approximate with another column
- you can approximate with that column from other rows


```{code-cell} ipython3
coffee_df['lot_number_clean'] = coffee_df['Lot.Number'].fillna(1)
```

```{code-cell} ipython3
coffee_df['lot_number_clean'] = coffee_df['Lot.Number'].fillna('1')
```

```{code-cell} ipython3
coffee_df.head()
```

```{code-cell} ipython3
coffee_df.shape
```

Dropping is a good choice when you otherwise have a lot of data and the data is
missing at random.

Dropping can be risky if it's not missing at random. For example, if we saw in
the coffee data that one of the scores was missing for all of the rows from one
country, or even just missing more often in one country, that could bias our
results.

```{code-cell} ipython3
coffee_df.dropna().shape
```

```{code-cell} ipython3
coffee_df.dropna(subset=['altitude_low_meters']).shape
```

**whatever you do, document it**


```{important}
Everththing after this is new material that we did not have time for in class,
but is important and helpful in your assignment (and for your portflio).
```

## Inconsistent values
---

```
df['In.Country.Partner'].value_counts().sort_index()
```


## Fixing data at load time

Let's explore some of the different parameters in `read_csv`

---
What parameter goes in the blank so that the following will make pandas read in the data in this image correctly?
![mulitindex img of excel file](https://github.com/rhodyprog4ds/BrownFall20/raw/main/img/multiindex.png)

```
pd.read_csv('fancy_formatting.xlsx', ___ = list(range(4)))
```

```{code-cell} ipython3

```

## A Cleaning Data Recipe

__not everything possible, but good enough for this course__


1. Can you use parameters to read the data in better?
1. Fix the index and column headers (making these easier to use makes the rest easier)
1. Is the data strucutred well?
1. Are there missing values?
1. Do the datatypes match what you expect by looking at the head or a sample?
1. Are categorical variables represented in usable way?
1. Does your analysis require filtering or augmenting the data?

Things to keep in mind:
- always save new copies of data when you mutate it
- add new columns rather than overwriting columns
- long variable names are better than ambiguous naming


## Your observations from Monday:

I promised we'd come back to your observations on what problems could occur in
data. Here they are, organized by rough categories of when/how to fix them.


We can fix while reading in data:
- decimal was indicated with ',' insead of '.' so pandas saw value as a string rather than a float
- missing header
- reading the index as a column
- large datasets might be too slow or not fit in memory
- missing data represeted with a value or special character

We can fix by reshaping data:
- Data can get read into tables in bizarre ways depending on how the data was entered originally.
- every value in one column, instead of separated

We can repair by changing values or filtering:
- information represented inconsistently eg "Value" and " Value " or twenty-two instead of 22
- blank rows or blank columns or data that is N/A
- date/time nformation can be represented lots of different ways
- representing categorical with numbers that are ambiguous
- spaces or other symbols in column names
- some numbers as strings, others as ints within a column
- symbols being mis intrepreted


Real problems, but beyond our scope:
- corrupt data files


## More Pracitce

Instead of more practice with these manipulations, below are more
examples of cleaning data to see how these types of manipulations get used.  
Your goal here is not to memorize every possible thing, but to build a general
idea of what good data looks like and good habits for cleaning data and keeping
it reproducible.  

Also here are some tips on general data management and organization.
