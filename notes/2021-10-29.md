---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

```{code-cell} ipython3
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import itertools as itr
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
sns.set_theme(font_scale=2,palette='colorblind')
```

```{code-cell} ipython3
tips = sns.load_dataset("tips").dropna()
```

```{code-cell} ipython3
tips.shape
```

```{code-cell} ipython3
tips.head()
```

```{code-cell} ipython3
# sklearn requires 2D object of features even for 1 feature
tips_X = tips['total_bill'].values
tips_X = tips_X[:,np.newaxis] # add an axis
tips_y = tips['tip']

tips_X_train,tips_X_test, tips_y_train, tips_y_test = train_test_split(
                     tips_X,
                     tips_y,
                     train_size=.8,
                     random_state=0)
```

```{code-cell} ipython3
regr_tips = linear_model.LinearRegression()
regr_tips.fit(tips_X_train,tips_y_train)
regr_tips.score(tips_X_test,tips_y_test)
```

This doesn't perform all that well, but let's investigate it futher. 
We'll start by looking at the residuals

```{code-cell} ipython3
tips_y_pred = regr_tips.predict(tips_X_test)
```

```{code-cell} ipython3
tips_y_pred - tips_y_test
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
```

```{code-cell} ipython3
tips_residuals = tips_y_pred - tips_y_test
plt.scatter(tips_X_test,tips_residuals, color='red')
```

```{code-cell} ipython3
poly = PolynomialFeatures(include_bias=False)
```

```{code-cell} ipython3
tips_X2_train = poly.fit_transform(tips_X_train)
tips_X2_test = poly.fit_transform(tips_X_test)
```

```{code-cell} ipython3
tips_X_train.shape, tips_X2_train.shape
```

```{code-cell} ipython3
tips_X2_train[:5,1]
```

```{code-cell} ipython3
tips_X_train[:5]
```

```{code-cell} ipython3
tips_X2_train[:5,0]
```

```{code-cell} ipython3
regr2_tips = linear_model.LinearRegression()
regr2_tips.fit(tips_X2_train,tips_y_train)
tips2_y_pred = regr2_tips.predict(tips_X2_test)
```

```{code-cell} ipython3
regr2_tips.coef_
```

```{code-cell} ipython3
regr_tips.coef_
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips2_y_pred, color='green')
```

```{code-cell} ipython3
tips.head()
```

```{code-cell} ipython3
tips.shape
```

We can also transform the other features to numerical features and then use that larger feature vector. 

```{code-cell} ipython3
bin_cols = ['total_bill','sex','smoker','time']
tips_bin = tips[bin_cols].replace({'Female':1,'Male':0,
                                   'No':0,'Yes':1,
                                  'Dinner':1,'Lunch':0})
tips_bin.shape
```

```{code-cell} ipython3

tips_onehot = pd.concat([tips['total_bill'],
                         pd.get_dummies(tips['sex']),
                         pd.get_dummies(tips['smoker']),
                         pd.get_dummies(tips['day']),
                        pd.get_dummies(tips['time'])],axis=1)

tips_onehot.head()
```

```{code-cell} ipython3
tips_onehot
```

```{code-cell} ipython3
col_names = (list(tips_onehot.columns) + 
[a+'_'+b + '_' +c +'_'+d for a,b,c,d in itr.combinations(tips_onehot.columns,4)])
```

```{code-cell} ipython3
col_names[11:90]
```

```{code-cell} ipython3
sns.lmplot(data=tips,x='total_bill',y='tip',
          col='sex',row='smoker',hue='time')
```

```{code-cell} ipython3
interaction = PolynomialFeatures(degree=,interaction_only=True, 
                                include_bias=False)
tips_interaction = interaction.fit_transform(tips_onehot,)
tips_all_X_train,tips_all_X_test, tips_all_y_train, tips_all_y_test = train_test_split(
                     tips_interaction,
                     tips_y,
                     train_size=.8,
                     random_state=0)
```

```{code-cell} ipython3
tips_all_X_train.shape
```

```{code-cell} ipython3
tips_all_X_train
```

```{code-cell} ipython3
regr_all_tips = linear_model.LinearRegression()
regr_all_tips.fit(tips_all_X_train,tips_all_y_train)
tips_all_y_pred = regr_all_tips.predict(tips_all_X_test,)
regr_all_tips.score(tips_all_X_test,tips_all_y_test)
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips_all_y_pred, color='green')
```

```{code-cell} ipython3
regr_all_tips.coef_
```

```{code-cell} ipython3
tips_lasso = linear_model.Lasso(alpha=.0025)
tips_lasso.fit(tips_all_X_train,tips_all_y_train)
tips_lasso_y_pred = tips_lasso.predict(tips_all_X_test,)
tips_lasso.score(tips_all_X_test,tips_all_y_test)
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips_lasso_y_pred, color='green')
```

```{code-cell} ipython3
sum(tips_lasso.coef_ ==0)/len(tips_lasso.coef_)
```

```{code-cell} ipython3
tips_onehot.shape, tips_interacion.shape
```

The transform changed our data from 10 columns to 55. 

```{code-cell} ipython3
tips_interacion.head
```

work through dummies

+++

show lasso

```{code-cell} ipython3

```
