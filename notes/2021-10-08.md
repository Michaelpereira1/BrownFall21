---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Web Scraping

```{code-cell} ipython3
import requests
from bs4 import BeautifulSoup
import pandas as pd
```


````{admonition} Tip
To update a package while running jupyter
in a notebook:
```
!pip install packagename update
```
then restart the kernel
````

## Figuring out what to scrape

We're going to create a DataFrame about URI CS & Statistics Faculty.

from the [people page](https://web.uri.edu/cs/people/) of the department website.


With great power comes great responsibility.

- always check [robots.txt](https://web.uri.edu/robots.txt)
- do not do things that the owner says not to do
- government websites are typically safe, because of open data rules


We can inspect the page to check that it's well structured by right clicking in
a browser tab and looking at the same code that our browser sees in order to
render the page.  

````{margin}
```{admonition} Think Ahead
You can use this same basic logic, that anything can be data to consider other
questions like, for example:
- How do the sizes of datasets for Tidy Tuesday vary? (you don't need to download and load all of the data, only traverse the readmes)
- On average, how many code cells are there in each class? How much does the amount of code in the posted notes vary from your notes you take in class?

```
````
We can basically think of web scraping as loading data that's *not* tabular but
instead is formatted as html code.  HTML code *can* be well strucutured and
hierarchical within a single page, or you could collect information about a
broad topic by using a little bit from many many pages.  We're going to work
from one page here.

HTML code consists of tags and the text of the page. The tags label the content
and define different structure of the page.  

![HTML tree structure](http://www.w3schools.com/js/pic_htmltree.gif)

Once we've decided that it will work, we can being working.
```{code-cell} ipython3
cs_people_url ='https://web.uri.edu/cs/people/'
```

## Loading with requests



First, we `get` the data using the python [requests](https://docs.python-requests.org/en/latest/)
library.


```{code-cell} ipython3
requests.get(cs_people_url)
```

this returns an object, but we want the content from it, so we'll save that to a
variable

```{code-cell} ipython3
cs_people_html = requests.get(cs_people_url).content

cs_people_html
```

This is literally just the html as a browser would see, but we pulled it into
python.


## Parsing with BeautifulSoup

 Next, we'll use BeautifulSoup to parse the text.  Parsing means to make
sense ot if.  It changes it from a string or characters, to a datastructure
that we can work with.

```{code-cell} ipython3
cs_people = BeautifulSoup(cs_people_html,'html.parser')
```

First we note that it now formats the new lines.
```{code-cell} ipython3
cs_people
```

We can use `prettify` method to format it more nicely.  This would add tabs even
if there were none in the source code.
```{code-cell} ipython3
print(cs_people.prettify())
```

It also makes the tags (strucutre in HTML) attributes.

```{code-cell} ipython3
cs_people.title
```

```{code-cell} ipython3
cs_people.a
```

We can use `find_all` to make a list of all occurences of a tag. For example, we
could get all of the links from a page:

```{code-cell} ipython3
cs_people.find_all('a')
```

We noted before that each person's information is contained in a `div` tag with
the `class = peopleitem`.  `find_all` can also take values for attributs of a
tag.  Attributes are the modifiers of a tag, in this case, a class is a label
that defines formatting, and in this case, acts as metadata about what is in the
div.  Scraping relies on the HTML code being well organized.  

```{code-cell} ipython3
cs_people.find_all("div","peopleitem")
```
We could use this to see how many people there are
```{code-cell} ipython3
len(cs_people.find_all("div","peopleitem"))
```
or use multiple to see how many people have  thumbnail
```{code-cell} ipython3
len(cs_people.find_all("div",{"has-thumbnail"}))
```
L
```{code-cell} ipython3
cs_people.find_all("div","peopleitem")[0]
```

```{code-cell} ipython3
first_name = cs_people.find_all("h3","p-name")[0]
```

```{code-cell} ipython3
type(first_name)
```

```{code-cell} ipython3
first_name.contents
```

```{code-cell} ipython3
first_name.string
```

```{code-cell} ipython3
[type(c) for c in first_name.children]
```

```{code-cell} ipython3
first_name.a
```

```{code-cell} ipython3
first_name.a.attrs
```

```{code-cell} ipython3
first_name.a.attrs.href
```

```{code-cell} ipython3
first_name.a.attrs['href']
```

```{code-cell} ipython3
names = [name.string for name in cs_people.find_all("h3","p-name")]
names
```

```{code-cell} ipython3
cs_people.find_all("div","peopleitem")[0]
```

```{code-cell} ipython3
cs_people.find_all("div","peopleitem")[0].img.attrs
```

```{code-cell} ipython3
cs_people.find_all("p")
```

```{code-cell} ipython3
disciplines = [d.string for d in cs_people.find_all("p",
                                                    'people-department')]
```

```{code-cell} ipython3
titles = [t.string for t in cs_people.find_all("p","people-title")]
emails = [e.string for e in cs_people.find_all("a",'u-email')]
pd.DataFrame({'name':names, 'title':titles,
              'e-mails':emails, 'discipline':disciplines})
```

```{code-cell} ipython3
sp22_csc_sta_url = 'https://raw.githubusercontent.com/rhodyprog4ds/rhodyds/main/data/reg_CSCSTA_courses.csv'
```

```{code-cell} ipython3
courses_df = pd.read_csv(sp22_csc_sta_url)
courses_df.head()
```

```{code-cell} ipython3

```
