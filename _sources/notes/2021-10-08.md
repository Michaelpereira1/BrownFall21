---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Web Scraping

```{code-cell} ipython3
import requests
from bs4 import BeautifulSoup
import pandas as pd
```


````{admonition} Tip
To update a package while running jupyter
in a notebook:
```
!pip install packagename update
```
then restart the kernel
````

## Figuring out what to scrape

We're going to create a DataFrame about URI CS & Statistics Faculty.

from the [people page](https://web.uri.edu/cs/people/) of the department website.


With great power comes great responsibility.

- always check [robots.txt](https://web.uri.edu/robots.txt)
- do not do things that the owner says not to do
- government websites are typically safe, because of open data rules


We can inspect the page to check that it's well structured by right clicking in
a browser tab and looking at the same code that our browser sees in order to
render the page.  

````{margin}
```{admonition} Think Ahead
You can use this same basic logic, that anything can be data to consider other
questions like, for example:
- How do the sizes of datasets for Tidy Tuesday vary? (you don't need to download and load all of the data, only traverse the readmes)
- On average, how many code cells are there in each class? How much does the amount of code in the posted notes vary from your notes you take in class?

```
````
We can basically think of web scraping as loading data that's *not* tabular but
instead is formatted as html code.  HTML code *can* be well strucutured and
hierarchical within a single page, or you could collect information about a
broad topic by using a little bit from many many pages.  We're going to work
from one page here.

HTML code consists of tags and the text of the page. The tags label the content
and define different structure of the page.  

![HTML tree structure](http://www.w3schools.com/js/pic_htmltree.gif)

Once we've decided that it will work, we can being working.
```{code-cell} ipython3
cs_people_url ='https://web.uri.edu/cs/people/'
```

## Loading with requests



First, we `get` the data using the python [requests](https://docs.python-requests.org/en/latest/)
library.


```{code-cell} ipython3
requests.get(cs_people_url)
```

this returns an object, but we want the content from it, so we'll save that to a
variable

```{code-cell} ipython3
cs_people_html = requests.get(cs_people_url).content

cs_people_html
```

This is literally just the html as a browser would see, but we pulled it into
python.


## Parsing with BeautifulSoup

 Next, we'll use BeautifulSoup to parse the text.  Parsing means to make
sense of it. In this case, it transforms from a string or characters, to a datastructure
that we can work with.

```{code-cell} ipython3
cs_people = BeautifulSoup(cs_people_html,'html.parser')
```

First we note that it now formats the new lines.
```{code-cell} ipython3
cs_people
```

We can use `prettify` method to format it more nicely.  This would add tabs even
if there were none in the source code.
```{code-cell} ipython3
print(cs_people.prettify())
```

It also makes the tags (structure in HTML) attributes.

```{code-cell} ipython3
cs_people.title
```

For tags that have multiple instances like the `<a>` tag that defines a link, it returns the first instance.
```{code-cell} ipython3
cs_people.a
```


## Finding all instances of a tag

We can use `find_all` to make a list of all occurences of a tag. For example, we
could get all of the links from a page:

```{code-cell} ipython3
cs_people.find_all('a')
```

We noted before that each person's information is contained in a `div` tag with
the `class = peopleitem`.  `find_all` can also take values for attributs of a
tag.  Attributes are the modifiers of a tag, in this case, a class is a label
that defines formatting, and in this case, acts as metadata about what is in the
div.  Scraping relies on the HTML code being well organized.  

```{code-cell} ipython3
cs_people.find_all("div","peopleitem")
```
We could use this to see how many people there are
```{code-cell} ipython3
len(cs_people.find_all("div","peopleitem"))
```
or use multiple to see how many people have  thumbnail
```{code-cell} ipython3
len(cs_people.find_all("div",{"has-thumbnail"}))
```

## Finding data we can make tabular

We can look at the first one in detail to determine what to extract for each of the column.
```{code-cell} ipython3
cs_people.find_all("div","peopleitem")[0]
```

We can see that the name is an `<h3>` tag with `class = "p-name"`
```{code-cell} ipython3
first_name = cs_people.find_all("h3","p-name")[0]
```

We can examine this using our typical tools:
```{code-cell} ipython3
type(first_name)
```

It has attributes, since it's a tag object:
```{code-cell} ipython3
first_name.contents
```

What we want is the string:
```{code-cell} ipython3
first_name.string
```

```{admonition} Question in class
How can we extract the link?
```

That's a child, because the `<a>` tag is inside of the the `<h3>` tag, so let's explore the children.
```{code-cell} ipython3
[type(c) for c in first_name.children]
```

Alternatively, we can pick the `a` tag out by name.
```{code-cell} ipython3
first_name.a
```

the url or `href` is an attribute of the `a` tag.
```{code-cell} ipython3
first_name.a.attrs
```

```{code-cell} ipython3
:tag: ["raises-exception"]
first_name.a.attrs.href
```

we can get it out using the `[]` to index into the `attrs` dictionary
```{code-cell} ipython3
first_name.a.attrs['href']
```

## Bulding a DataFrame

Now that we know what to look for, we can start building.  First, we'll find all of the names and extract the string from each using a list comprehension.

```{code-cell} ipython3
names = [name.string for name in cs_people.find_all("h3","p-name")]
names
```

We can use the same process for each other attribut we want.  First, we'll
look at the whole peopleitem again , and then decide what we want.


```{code-cell} ipython3
cs_people.find_all("div","peopleitem")[0]
```

We'll extract the department, title, and e-mail.

```{code-cell} ipython3
disciplines = [d.string for d in cs_people.find_all("p",
                                                    'people-department')]
titles = [t.string for t in cs_people.find_all("p","people-title")]
emails = [e.string for e in cs_people.find_all("a",'u-email')]
pd.DataFrame({'name':names, 'title':titles,
              'e-mails':emails, 'discipline':disciplines})
```

```{code-cell} ipython3
sp22_csc_sta_url = 'https://raw.githubusercontent.com/rhodyprog4ds/rhodyds/main/data/reg_CSCSTA_courses.csv'
```

```{code-cell} ipython3
courses_df = pd.read_csv(sp22_csc_sta_url)
courses_df.head()
```

We saw the `attrs` for a link above, where there was only one attribute on the tag, but for example, the images on each person's card have many attributes.
```{code-cell} ipython3
cs_people.find_all("div","peopleitem")[0].img.attrs
```


```{code-cell} ipython3
cs_people.find_all("p")
```

URI websites are probably formatted consistently, so we could build information about more departments.

- [csc/sta emeriti](https://web.uri.edu/cs/people/faculty-emeriti/)
- [a&s dean's office](https://web.uri.edu/artsci/people/)
- [math](https://www.math.uri.edu/people/)
- [philosophy](https://web.uri.edu/philosophy/people/)
- [business](https://web.uri.edu/business/people/faculty/)



## Thinking Ahead


The spreadsheet of spring classes in the department is posted:

```
sp22_csc_sta_url = 'https://raw.githubusercontent.com/rhodyprog4ds/rhodyds/main/data/reg_CSCSTA_courses.csv'
```

this is a minimal copy where I removed enrollments and locations in case those change.
this is derived from the last version the Dean asked us to make corrections to, so things
will definitely be different before registration opens (eg I'm teaching the
CSC392 and it's listed as "Staff")

sections have definitely been added/removed and teaching assignments changed, so
don't use this for making plans.

How could you merge this with the DataFrame we just scraped?




## More Practice

1. Add a phone number column
1. On the page linked from each person's name, their office number; add a column for office number.
1. Make the code we wrote in class into a function so that you can pass a page and get back a DataFrame.
1. Parse the [course descriptions](https://web.uri.edu/cs/academics/computer-science/course-descriptions/)  page and make a DataFrame with columns for subject code, course number, course title, and course description.
1. Merge the descriptions with the table about enrollments and instructors.
